{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-15T23:25:53.868129Z","iopub.execute_input":"2024-04-15T23:25:53.868527Z","iopub.status.idle":"2024-04-15T23:25:54.954788Z","shell.execute_reply.started":"2024-04-15T23:25:53.868491Z","shell.execute_reply":"2024-04-15T23:25:54.953549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Introduction**\n\nNatural Language Processing, also known as NLP, is a subfield of computer science, specifically artificial intelligence, that focuses on understanding written and spoken text. This project covers various tasks of disaster tweets. I applied some Data mining skill set, via Data Understanding, Data Pre-processing, Data Werehousing, Data Modeling and Data Evaluation techniques. \n\n","metadata":{}},{"cell_type":"markdown","source":"**Describe the Data** The train and test data are structured labelled data that are imported from CSV files in the form of pandas DataFrame using pr.read csv method (Pyhon package).\n\nAs seen in the figure below, the DataFrame is made up of the following 4 columns:\n* id: a unique identifier of every tweet\n* keyword: a particular keyword from the tweet (this can be blank)\n* location: the location the tweet was sent from (this can be blank)\n* text: the text of the tweet\n* target: present only in the train data, and denotes if the tweet is about a real disaster (1) or not (0)","metadata":{}},{"cell_type":"code","source":"# For Preprocesssing Text Data\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Feature Extraction\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Import the train test split\nfrom sklearn.model_selection import train_test_split\n\n# Check Performance\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:27:07.433094Z","iopub.execute_input":"2024-04-15T23:27:07.433485Z","iopub.status.idle":"2024-04-15T23:27:09.625423Z","shell.execute_reply.started":"2024-04-15T23:27:07.433454Z","shell.execute_reply":"2024-04-15T23:27:09.624440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Train=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\nlen(disaster_Train)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:28:08.263508Z","iopub.execute_input":"2024-04-15T23:28:08.263907Z","iopub.status.idle":"2024-04-15T23:28:08.318185Z","shell.execute_reply.started":"2024-04-15T23:28:08.263875Z","shell.execute_reply":"2024-04-15T23:28:08.317288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Test=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nlen(disaster_Test)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:28:37.571134Z","iopub.execute_input":"2024-04-15T23:28:37.571585Z","iopub.status.idle":"2024-04-15T23:28:37.602237Z","shell.execute_reply.started":"2024-04-15T23:28:37.571550Z","shell.execute_reply":"2024-04-15T23:28:37.601080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:28:52.964925Z","iopub.execute_input":"2024-04-15T23:28:52.966120Z","iopub.status.idle":"2024-04-15T23:28:52.997747Z","shell.execute_reply.started":"2024-04-15T23:28:52.966080Z","shell.execute_reply":"2024-04-15T23:28:52.996710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Train.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:29:05.320613Z","iopub.execute_input":"2024-04-15T23:29:05.321041Z","iopub.status.idle":"2024-04-15T23:29:05.337587Z","shell.execute_reply.started":"2024-04-15T23:29:05.321005Z","shell.execute_reply":"2024-04-15T23:29:05.336369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Text Preprocessing ** **Exploratory Data Analysis (EDA), Visualize and Clean the Dataset**\n\nData preprocessing involves, cleaning the dataset and preparing text data before encoding them in the form of numeric vectors.\n\n* Identify the missing value in the Dataset\n* Entities, URL Links and Punctuation Removal\n* Spelling Correction\n* Filling Missing Data by Keyword Extraction and Entity Recognition\n* Lemmatization\n* Stop Words Removal","metadata":{}},{"cell_type":"code","source":"# Data information\ndisaster_Train.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:30:06.384102Z","iopub.execute_input":"2024-04-15T23:30:06.384524Z","iopub.status.idle":"2024-04-15T23:30:06.407957Z","shell.execute_reply.started":"2024-04-15T23:30:06.384490Z","shell.execute_reply":"2024-04-15T23:30:06.406744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize**","metadata":{}},{"cell_type":"code","source":"disaster_Train['target'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:30:37.568089Z","iopub.execute_input":"2024-04-15T23:30:37.568502Z","iopub.status.idle":"2024-04-15T23:30:37.814486Z","shell.execute_reply.started":"2024-04-15T23:30:37.568466Z","shell.execute_reply":"2024-04-15T23:30:37.813219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Identify the missing value in the Dataset\nprint(disaster_Train.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:30:53.180544Z","iopub.execute_input":"2024-04-15T23:30:53.180918Z","iopub.status.idle":"2024-04-15T23:30:53.189558Z","shell.execute_reply.started":"2024-04-15T23:30:53.180888Z","shell.execute_reply":"2024-04-15T23:30:53.188309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Train['keyword'].fillna('empty', inplace=True)\ndisaster_Train['location'].fillna('empty', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:31:06.503233Z","iopub.execute_input":"2024-04-15T23:31:06.504284Z","iopub.status.idle":"2024-04-15T23:31:06.513004Z","shell.execute_reply.started":"2024-04-15T23:31:06.504241Z","shell.execute_reply":"2024-04-15T23:31:06.511864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(disaster_Train.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:31:21.298912Z","iopub.execute_input":"2024-04-15T23:31:21.299302Z","iopub.status.idle":"2024-04-15T23:31:21.307682Z","shell.execute_reply.started":"2024-04-15T23:31:21.299271Z","shell.execute_reply":"2024-04-15T23:31:21.306483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lowercasing**\n\nConverting the text into lowercase is an essential step in any NLP project. if not converted to lowercase, they will be represented as three different words in the vector space model","metadata":{}},{"cell_type":"code","source":"#convert to lowercase on Train \ndisaster_Train[\"keyword\"] = disaster_Train[\"keyword\"].apply(lambda x: str.lower(x))\ndisaster_Train[\"location\"] = disaster_Train[\"location\"].apply(lambda x: str.lower(x))\ndisaster_Train[\"text\"] = disaster_Train[\"text\"].apply(lambda x: str.lower(x))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:31:55.227441Z","iopub.execute_input":"2024-04-15T23:31:55.227839Z","iopub.status.idle":"2024-04-15T23:31:55.246534Z","shell.execute_reply.started":"2024-04-15T23:31:55.227806Z","shell.execute_reply":"2024-04-15T23:31:55.245514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is important to pay attention to the fact that the 'keyword' and 'location' columns contain missing values (i.e.: values that are NA such as numpy.nan or None); to avoid an unexpected error, inplace method is used to skip empty entries.","metadata":{}},{"cell_type":"code","source":"disaster_Test['keyword'].fillna('empty', inplace=True)\ndisaster_Test['location'].fillna('empty', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:32:20.826019Z","iopub.execute_input":"2024-04-15T23:32:20.826381Z","iopub.status.idle":"2024-04-15T23:32:20.834743Z","shell.execute_reply.started":"2024-04-15T23:32:20.826351Z","shell.execute_reply":"2024-04-15T23:32:20.833531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert to lowercase on Test\ndisaster_Test[\"keyword\"] = disaster_Test[\"keyword\"].apply(lambda x: str.lower(x))\ndisaster_Test[\"location\"] = disaster_Test[\"location\"].apply(lambda x: str.lower(x))\ndisaster_Test[\"text\"] = disaster_Test[\"text\"].apply(lambda x: str.lower(x))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:32:37.367936Z","iopub.execute_input":"2024-04-15T23:32:37.368439Z","iopub.status.idle":"2024-04-15T23:32:37.385228Z","shell.execute_reply.started":"2024-04-15T23:32:37.368379Z","shell.execute_reply":"2024-04-15T23:32:37.383923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Entities, URL Links and Punctuation Removal**\n\nSince the dataset is based on tweets, it may include a lot of mentions (e.g.: @somone02) and hashtags (e.g.: #). These are removed by creating a function named remove_entities","metadata":{}},{"cell_type":"code","source":"import re, string\ndef remove_entities(text):\n    entity_prefixes = ['@','#']\n    for seperator in string.puntuation:\n        if seperator not in entity_prefixes :\n            text = text.replace(separator, ' ')\n    words = []\n    for word in text.split():\n        word = word.strip()\n        if word:\n            if word(0) not in entity_prefixes:\n                words.append(word)\n    return ' '.join(words)\n\ndisaster_Train[\"keyword\"] = disaster_Train[\"keyword\"].apply(lambda x: str.lower(x))\ndisaster_Train[\"location\"] = disaster_Train[\"location\"].apply(lambda x: str.lower(x))\ndisaster_Train[\"text\"] = disaster_Train[\"text\"].apply(lambda x: str.lower(x))\n\ndisaster_Test[\"keyword\"] = disaster_Test[\"keyword\"].apply(lambda x: str.lower(x))\ndisaster_Test[\"location\"] = disaster_Test[\"location\"].apply(lambda x: str.lower(x))\ndisaster_Test[\"text\"] = disaster_Test[\"text\"].apply(lambda x: str.lower(x))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:33:13.911323Z","iopub.execute_input":"2024-04-15T23:33:13.911699Z","iopub.status.idle":"2024-04-15T23:33:13.935943Z","shell.execute_reply.started":"2024-04-15T23:33:13.911669Z","shell.execute_reply":"2024-04-15T23:33:13.934313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replace URL links with Blank","metadata":{}},{"cell_type":"code","source":"import re\n\ndisaster_Train[\"keyword\"] = disaster_Train[\"keyword\"].apply(lambda x: re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\",' ', x))\ndisaster_Train[\"location\"] = disaster_Train[\"location\"].apply(lambda x: re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\",' ', x))\ndisaster_Train[\"text\"] = disaster_Train[\"text\"].apply(lambda x: re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\",' ', x))\n\ndisaster_Test[\"keyword\"] = disaster_Test[\"keyword\"].apply(lambda x: re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\",' ', x))\ndisaster_Test[\"location\"] = disaster_Test[\"location\"].apply(lambda x: re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\",' ', x))\ndisaster_Test[\"text\"] = disaster_Test[\"text\"].apply(lambda x: re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\",' ', x))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:33:41.514077Z","iopub.execute_input":"2024-04-15T23:33:41.514554Z","iopub.status.idle":"2024-04-15T23:33:41.589431Z","shell.execute_reply.started":"2024-04-15T23:33:41.514519Z","shell.execute_reply":"2024-04-15T23:33:41.588381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove puntuation**","metadata":{}},{"cell_type":"code","source":"import re\n\ndisaster_Train[\"keyword\"] = disaster_Train[\"keyword\"].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\ndisaster_Train[\"location\"] = disaster_Train[\"location\"].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\ndisaster_Train[\"text\"] = disaster_Train[\"text\"].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n\ndisaster_Test[\"keyword\"] = disaster_Test[\"keyword\"].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\ndisaster_Test[\"location\"] = disaster_Test[\"location\"].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\ndisaster_Test[\"text\"] = disaster_Test[\"text\"].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:34:10.995080Z","iopub.execute_input":"2024-04-15T23:34:10.995479Z","iopub.status.idle":"2024-04-15T23:34:11.079846Z","shell.execute_reply.started":"2024-04-15T23:34:10.995448Z","shell.execute_reply":"2024-04-15T23:34:11.079025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Spelling Correction.**\n\nIt is expected that tweets will include several spelling mistakes, therefore spelling correction is applied to improve performance","metadata":{}},{"cell_type":"code","source":"pip install -U symspellpy","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:34:42.730676Z","iopub.execute_input":"2024-04-15T23:34:42.731764Z","iopub.status.idle":"2024-04-15T23:35:16.212132Z","shell.execute_reply.started":"2024-04-15T23:34:42.731709Z","shell.execute_reply":"2024-04-15T23:35:16.210567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pkg_resources","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:35:34.452584Z","iopub.execute_input":"2024-04-15T23:35:34.453007Z","iopub.status.idle":"2024-04-15T23:35:34.735710Z","shell.execute_reply.started":"2024-04-15T23:35:34.452969Z","shell.execute_reply":"2024-04-15T23:35:34.734608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pkg_resources\n\ndictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\nbigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:35:40.958339Z","iopub.execute_input":"2024-04-15T23:35:40.958762Z","iopub.status.idle":"2024-04-15T23:35:40.971597Z","shell.execute_reply.started":"2024-04-15T23:35:40.958728Z","shell.execute_reply":"2024-04-15T23:35:40.970642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from symspellpy import SymSpell, Verbosity\nsym_spell = SymSpell()\n#dictionary_path = \"./frequency_dictionary_en_82_765.txt\"\nsym_spell.load_dictionary(dictionary_path, 0, 1)\n\ndef spelling_correction(sent):\n    doc_w_correct_spelling = []\n    for tok in sent.split(\" \"):\n        x= sym_spell.lookup(tok, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)[0].__str__()\n        y= x.split(',')[0]\n        doc_w_correct_spelling.append(y)\n    return \" \".join(doc_w_correct_spelling)\n\ndisaster_Train[\"keyword\"] = disaster_Train[\"keyword\"].apply(lambda x: spelling_correction(x))\ndisaster_Train[\"location\"] = disaster_Train[\"location\"].apply(lambda x: spelling_correction(x))\ndisaster_Train[\"text\"] = disaster_Train[\"text\"].apply(lambda x: spelling_correction(x))\n\ndisaster_Test[\"keyword\"] = disaster_Test[\"keyword\"].apply(lambda x: spelling_correction(x))\ndisaster_Test[\"location\"] = disaster_Test[\"location\"].apply(lambda x: spelling_correction(x))\ndisaster_Test[\"text\"] = disaster_Test[\"text\"].apply(lambda x: spelling_correction(x))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:35:58.933316Z","iopub.execute_input":"2024-04-15T23:35:58.933694Z","iopub.status.idle":"2024-04-15T23:36:08.638895Z","shell.execute_reply.started":"2024-04-15T23:35:58.933665Z","shell.execute_reply":"2024-04-15T23:36:08.637807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Filling Missing Data**\n\ntrain.info() and test.info() reveal that the train data has 61 entries empty in the keyword column, and 2533 entries empty in the location column; And, the test data has 26 entries empty in the keyword column, and 1105 entries empty in the location column\n\n**Keyword Extraction** The extract_keywords function, shown below, was inspired by the following articles Keyword Extraction with BERT and Build A Keyword Extraction API with Spacy, Flask, and FuzzyWuzzy","metadata":{}},{"cell_type":"code","source":"pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:36:39.684588Z","iopub.execute_input":"2024-04-15T23:36:39.684975Z","iopub.status.idle":"2024-04-15T23:36:52.438660Z","shell.execute_reply.started":"2024-04-15T23:36:39.684945Z","shell.execute_reply":"2024-04-15T23:36:52.437198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:37:03.295008Z","iopub.execute_input":"2024-04-15T23:37:03.295443Z","iopub.status.idle":"2024-04-15T23:37:10.709089Z","shell.execute_reply.started":"2024-04-15T23:37:03.295402Z","shell.execute_reply":"2024-04-15T23:37:10.707807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download the language model\nimport os\nos.system('python -m spacy download en')\nnlp = spacy.blank(\"en\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:37:18.125362Z","iopub.execute_input":"2024-04-15T23:37:18.126024Z","iopub.status.idle":"2024-04-15T23:37:33.611118Z","shell.execute_reply.started":"2024-04-15T23:37:18.125987Z","shell.execute_reply":"2024-04-15T23:37:33.609701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_hub as hub\nmodel = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\nembed = hub.load(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:37:55.688715Z","iopub.execute_input":"2024-04-15T23:37:55.689442Z","iopub.status.idle":"2024-04-15T23:38:12.507040Z","shell.execute_reply.started":"2024-04-15T23:37:55.689406Z","shell.execute_reply":"2024-04-15T23:38:12.505732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract keywords\n\ndef extract_keywords(nlp=nlp, doc=\"\", no_of_keywords=5, model=model):\n    doc = doc.lower()\n    doc = re.sub(r'(?:\\@|http?\\://|https?\\://|www)\\S+',' ', doc)\n    doc = re.sub(r'[^\\w\\s]',' ', doc)\n    doc = re.sub(' \\d+', ' ', doc)\n    doc_ = nlp(doc)\n    \n    pos_tag = ['VERB','NOUN','ADJ','PROPN']\n    result=[]\n    \n    for token in doc_:\n        if (token.pos_ in pos_tag):\n            result.append(token.text)\n            \n    doc_embedding = model.encode([doc])\n    results_embeddings = model.encoded(result)\n    \n    distances = cosine_similarity(doc_embedding, results_embeddings)\n    keywords = [result[index] for index in distances.argsort()[0][-no_of_keywords:]]\n    return keywords\nfor i in range(len(disaster_Train[\"keyword\"])):\n    if pd.isnull(disaster_Train['keyword'].iloc[i]):\n        try:\n            disaster_Train['keyword'].iloc[i] = extract_keywords(nlp=nlps, doc=disaster_Train.text.iloc[i], no_of_keywords=1, model=model)[0]\n        except:\n            disaster_Train['keyword'].iloc[i] = \"NaN\"\n            \nfor i in range(len(disaster_Test[\"keyword\"])):\n    if pd.isnull(disaster_Test['keyword'].iloc[i]):\n        try:\n            disaster_Test['keyword'].iloc[i] = extract_keywords(nlp=nlps, doc=disaster_Test.text.iloc[i], no_of_keywords=1, model=model)[0]\n        except:\n            disaster_Test['keyword'].iloc[i] = \"NaN\"\n            ","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:38:19.086153Z","iopub.execute_input":"2024-04-15T23:38:19.086893Z","iopub.status.idle":"2024-04-15T23:38:19.238820Z","shell.execute_reply.started":"2024-04-15T23:38:19.086844Z","shell.execute_reply":"2024-04-15T23:38:19.237525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stop Words Removal**\n\nStop words provide low level information to the text and are often found in abundance; therefore, they are removed to give more focus to other significant information","metadata":{}},{"cell_type":"code","source":"nltk.download('stopwords')\nstop = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:38:55.592349Z","iopub.execute_input":"2024-04-15T23:38:55.592761Z","iopub.status.idle":"2024-04-15T23:38:55.688138Z","shell.execute_reply.started":"2024-04-15T23:38:55.592730Z","shell.execute_reply":"2024-04-15T23:38:55.686942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Train['text'].apply(lambda x: [word for word in x.split() if word not in stop])\ndisaster_Train['keyword'].apply(lambda x: [word for word in x.split() if word not in stop])\ndisaster_Train['location'].apply(lambda x: [word for word in x.split() if word not in stop])","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:39:11.806014Z","iopub.execute_input":"2024-04-15T23:39:11.807062Z","iopub.status.idle":"2024-04-15T23:39:12.145163Z","shell.execute_reply.started":"2024-04-15T23:39:11.807011Z","shell.execute_reply":"2024-04-15T23:39:12.144017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Test['text'].apply(lambda x: [word for word in x.split() if word not in stop])\ndisaster_Test['keyword'].apply(lambda x: [word for word in x.split() if word not in stop])\ndisaster_Test['location'].apply(lambda x: [word for word in x.split() if word not in stop])","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:39:27.557757Z","iopub.execute_input":"2024-04-15T23:39:27.558404Z","iopub.status.idle":"2024-04-15T23:39:27.711162Z","shell.execute_reply.started":"2024-04-15T23:39:27.558353Z","shell.execute_reply":"2024-04-15T23:39:27.710246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Train['text'] = disaster_Train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndisaster_Train['keyword'] = disaster_Train['keyword'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndisaster_Train['location'] = disaster_Train['location'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:39:49.774069Z","iopub.execute_input":"2024-04-15T23:39:49.774706Z","iopub.status.idle":"2024-04-15T23:39:50.096838Z","shell.execute_reply.started":"2024-04-15T23:39:49.774671Z","shell.execute_reply":"2024-04-15T23:39:50.095749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Test['text'] = disaster_Test['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndisaster_Test['keyword'] = disaster_Test['keyword'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndisaster_Test['location'] = disaster_Test['location'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:40:04.184305Z","iopub.execute_input":"2024-04-15T23:40:04.184744Z","iopub.status.idle":"2024-04-15T23:40:04.334815Z","shell.execute_reply.started":"2024-04-15T23:40:04.184713Z","shell.execute_reply":"2024-04-15T23:40:04.333727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Train.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:40:14.671869Z","iopub.execute_input":"2024-04-15T23:40:14.672871Z","iopub.status.idle":"2024-04-15T23:40:14.686073Z","shell.execute_reply.started":"2024-04-15T23:40:14.672830Z","shell.execute_reply":"2024-04-15T23:40:14.684617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Test.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:40:25.606025Z","iopub.execute_input":"2024-04-15T23:40:25.606825Z","iopub.status.idle":"2024-04-15T23:40:25.617762Z","shell.execute_reply.started":"2024-04-15T23:40:25.606791Z","shell.execute_reply":"2024-04-15T23:40:25.616453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Develop summission code\ny = disaster_Train['target']\nX = disaster_Train.drop(columns=['target', 'id'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the train and test sets to verify\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:40:51.329229Z","iopub.execute_input":"2024-04-15T23:40:51.329662Z","iopub.status.idle":"2024-04-15T23:40:51.346828Z","shell.execute_reply.started":"2024-04-15T23:40:51.329631Z","shell.execute_reply":"2024-04-15T23:40:51.345371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:41:06.173333Z","iopub.execute_input":"2024-04-15T23:41:06.173769Z","iopub.status.idle":"2024-04-15T23:41:06.186096Z","shell.execute_reply.started":"2024-04-15T23:41:06.173734Z","shell.execute_reply":"2024-04-15T23:41:06.184882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transform to CountVectorizer**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=3000)\n\ncv.fit(X_train['text'])\nX_train_cv = cv.transform(X_train['text']).toarray()\nX_test_cv = cv.transform(X_test['text']).toarray()\n\nprint(type(X_train))\nprint(type(X_train_cv))\n\ntrain_with_cv = pd.DataFrame(X_train_cv, columns= cv.get_feature_names_out())\ntrain_with_cv.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:41:37.380699Z","iopub.execute_input":"2024-04-15T23:41:37.381120Z","iopub.status.idle":"2024-04-15T23:41:37.703860Z","shell.execute_reply.started":"2024-04-15T23:41:37.381091Z","shell.execute_reply":"2024-04-15T23:41:37.702551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Architecture**\n\nDevelop the model with sklearn naive bayes package","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel_disaster=MultinomialNB().fit(X_train_cv,y_train)\n\ny_train_pred = model_disaster.predict(X_train_cv)\ny_test_pred = model_disaster.predict(X_test_cv)\n\nprint('Train Report ---')\nprint(classification_report(y_train, y_train_pred))\n\nprint('Validation Report ---')\nprint(classification_report(y_test, y_test_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:42:10.149084Z","iopub.execute_input":"2024-04-15T23:42:10.149534Z","iopub.status.idle":"2024-04-15T23:42:10.791569Z","shell.execute_reply.started":"2024-04-15T23:42:10.149499Z","shell.execute_reply":"2024-04-15T23:42:10.790270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test predicitons\nX_test_cv = cv.transform(disaster_Test['text']).toarray()\ny_test_pred = model_disaster.predict(X_test_cv)\n\n# Convert predictions to a DataFrame\nmodel_disaster1 = pd.DataFrame(y_test_pred)\nmodel_disaster1.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:42:25.709968Z","iopub.execute_input":"2024-04-15T23:42:25.710363Z","iopub.status.idle":"2024-04-15T23:42:25.858637Z","shell.execute_reply.started":"2024-04-15T23:42:25.710332Z","shell.execute_reply":"2024-04-15T23:42:25.857444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=3000)\n\ncv.fit(X_train['text'])\nX_train_cv = cv.transform(X_train['text']).toarray()\nX_test_cv = cv.transform(X_test['text']).toarray()\n\nprint(type(X_train))\nprint(type(X_train_cv))\n\ntrain_with_cv = pd.DataFrame(X_train_cv, columns= cv.get_feature_names_out())\ntrain_with_cv.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:42:44.500935Z","iopub.execute_input":"2024-04-15T23:42:44.501325Z","iopub.status.idle":"2024-04-15T23:42:44.793465Z","shell.execute_reply.started":"2024-04-15T23:42:44.501295Z","shell.execute_reply":"2024-04-15T23:42:44.792527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction code\npredictions_train = model_disaster.predict(X_train_cv)\npredictions_test = model_disaster.predict(X_test_cv)\n# Convert predictions to DataFrames\ndisaster1 = pd.DataFrame({'target': predictions_train})\ndisaster2 = pd.DataFrame({'target': predictions_test})\n\n# Concatenate DataFrames vertically\nsubmission = pd.concat([disaster1, disaster2], ignore_index=True)\nsubmission = pd.DataFrame(submission)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:43:01.575472Z","iopub.execute_input":"2024-04-15T23:43:01.575872Z","iopub.status.idle":"2024-04-15T23:43:01.740366Z","shell.execute_reply.started":"2024-04-15T23:43:01.575842Z","shell.execute_reply":"2024-04-15T23:43:01.739057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_Test=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsubmission = pd.DataFrame({'id': disaster_Test['id'], 'target': model_disaster1[0]})\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:43:56.825333Z","iopub.execute_input":"2024-04-15T23:43:56.826058Z","iopub.status.idle":"2024-04-15T23:43:56.855009Z","shell.execute_reply.started":"2024-04-15T23:43:56.826020Z","shell.execute_reply":"2024-04-15T23:43:56.853894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the DataFrame to a CSV file (adjust the filename as needed)\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T23:44:19.615624Z","iopub.execute_input":"2024-04-15T23:44:19.616287Z","iopub.status.idle":"2024-04-15T23:44:19.626514Z","shell.execute_reply.started":"2024-04-15T23:44:19.616253Z","shell.execute_reply":"2024-04-15T23:44:19.625460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**\nI improve This article by introduces natural language processing (NLP) through a sentiment analysis project. It focuses on text preprocessing, cleaning the data and model creation. The problem of the data is to clearn and work on Missing value for modeling creation\n\nThe text preprocessing is made up of six main steps which are: Lowercasing, Entities, URL Links and Punctuation Removal, Spelling Correction, Filling Missing Data, Lemmatization and Stop Words Removal. It is important to keep in mind that one can choose to include more steps to the text preprocessing (e.g. converting abbreviations to their original forms) or exclude some steps she/he thinks are unnecessary.\n\nIn the model section, the model reported accuracy 86%. The model can be used with logistic and decision tree model for more perfect accurancy. ","metadata":{}}]}